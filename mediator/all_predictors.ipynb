{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the command line in Savio, please activate the conda environment and install the allpredictors kernel as follows. This only needs to be done the very first time you use the notebook.\n",
    "\n",
    "`ml python/3.7; source activate /global/scratch/projects/fc_mvslab/conda/allpredictors`\n",
    "\n",
    "`python -m ipykernel install --user --name=allpredictors`\n",
    "\n",
    "Looking at the ribbon above, please click Kernel >> Change kernel >> allpredictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PADDLE\n",
      "  Downloading paddle-1.0.2.tar.gz (579 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m579.0/579.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[8 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/rq/l7tf3_nx0ml2t2rb_tgxvm280000gn/T/pip-install-79et0mdg/paddle_75efd220a77c49509d9af1affce8d754/setup.py\", line 3, in <module>\n",
      "  \u001b[31m   \u001b[0m     import paddle\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/rq/l7tf3_nx0ml2t2rb_tgxvm280000gn/T/pip-install-79et0mdg/paddle_75efd220a77c49509d9af1affce8d754/paddle/__init__.py\", line 5, in <module>\n",
      "  \u001b[31m   \u001b[0m     import common, dual, tight, data, prox\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'common'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install PADDLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipyparallel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mpickle\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mre\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msubprocess\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mcsv\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m uuid4\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipyparallel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mipp\u001b[39;00m\n\u001b[1;32m     18\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPADDLE\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m paddle\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipyparallel'"
     ]
    }
   ],
   "source": [
    "# Load necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "matplotlib.rcParams['font.sans-serif'] = 'Arial'\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"notebook\")\n",
    "from Bio import SeqIO\n",
    "import glob, pickle, os, re, subprocess, csv, sys\n",
    "from uuid import uuid4\n",
    "import ipyparallel as ipp\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from PADDLE import paddle\n",
    "from iupred3.iupred import get_iupred\n",
    "from adpred import ADpred as adp\n",
    "from TADA.src.Preprocessing import scale_features\n",
    "from TADA.src.Preprocessing import create_features\n",
    "from TADA.src.Predict_model_only import tada_predict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn import preprocessing\n",
    "from pickle import dump\n",
    "import pytorch_lightning as pl\n",
    "from actpred.models import ActCNNSystem\n",
    "from actpred.utils import get_threshold, get_stratified_split\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger, WandbLogger\n",
    "\n",
    "# Set Numpy to display floats with 3 decimal places\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run the predictors on many sequences, please see the BATCH section below the first STOP\n",
    "### To run the predictor on a single sequence, please run all cells until you see the first STOP\n",
    "\n",
    "<br>\n",
    "\n",
    "Please delete/move files in tmp_output after you're done. Files in tmp_output should not be expected to stay there permanently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SeqIO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c64a84bf9d7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfasta_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeqIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasta_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fasta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mfasta_name_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasta_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.fasta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.fa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfasta_name_tmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SeqIO' is not defined"
     ]
    }
   ],
   "source": [
    "# Input your single sequence here, or provide the path to a fasta file containing your sequence\n",
    "\n",
    "# INPUT PATH TO FASTA FILE HERE\n",
    "fasta_name = \"All_AD_seq/sequences.fasta\"\n",
    "# INPUT SEQUENCE HERE\n",
    "in_sequence = 'RLEQLFLLIFPREDLDMILKMDSLQDIKALLTGLFVQDNVNKDAVTDRLASVETDMPLTLRQHRISATSSSEESSNKGQRQLTVSIDSAAHHDNSTIPLDFMPRDALHGFDWSEEDDMSDGLPFLKTDPNNNGFFGDGSLLCILRSIGFKPENYTNSNVNRLPTMITDRYTLASRSTTSRLLQSYL'\n",
    "\n",
    "tmp_output = '/global/scratch/projects/fc_mvslab/predictors/all_predictors/tmp_output/dan'\n",
    "\n",
    "if fasta_name is not None:\n",
    "    sequence = str(list(SeqIO.parse(fasta_name, 'fasta'))[0].seq)\n",
    "    fasta_name_tmp = fasta_name.split('/')[-1].strip('.fasta').strip('.fa')\n",
    "    output_dir = tmp_output + '/' + fasta_name_tmp\n",
    "    subprocess.run(['mkdir', output_dir])\n",
    "    subprocess.run(['cp', fasta_name, output_dir])\n",
    "    fasta_name = fasta_name_tmp\n",
    "else:\n",
    "    sequence = in_sequence\n",
    "    fasta_name = str(uuid4())\n",
    "    print(fasta_name)   # Name of subdirectory under tmp_output containing predictions, etc.\n",
    "    output_dir = tmp_output + '/' + fasta_name\n",
    "    subprocess.run(['mkdir', output_dir])\n",
    "    with open(output_dir + '/' + fasta_name + '.fasta', 'w') as fa:\n",
    "        fa.write(sequence)\n",
    "\n",
    "assert len(sequence) >= 53, 'Protein sequence must be at least 53 amino acids long'    \n",
    "\n",
    "aa_lst = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "assert len(set(sequence) - set(aa_lst)) == 0, 'No non-standard amino acids allowed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSIPRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both PADDLE and ADpred need secondary structure predictions from PSIPRED. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to a local installation of psipred\n",
    "local_psipred = '/global/scratch/projects/fc_mvslab/predictors/psipred/runpsipred'\n",
    "\n",
    "\n",
    "def adpred_read_ss2(ss2):\n",
    "    '''\n",
    "    Converts .ss2 data to horizontal secondary structure information (e.g., --HHHHH--HHHHH-HH)\n",
    "    \n",
    "    ss2: name of .ss2 file\n",
    "    '''\n",
    "    df = pd.read_table(ss2, skiprows=[0], sep='\\s+', names=['position', 'residue', 'ss', 'c', 'h', 'e'])\n",
    "    return ''.join(df.ss.values).replace('C','-')\n",
    "\n",
    "def get_psipred(seq, output_dir=output_dir, fasta_name=fasta_name, computed=None):\n",
    "    '''\n",
    "    Runs PSIPRED on the provided sequence and returns secondary structure\n",
    "    data for PADDLE and ADpred, viz\n",
    "    (paddle_seq, paddle_helix, paddle_coil), 'adpred_ss_as_a_string'\n",
    "    \n",
    "    seq: provided sequence\n",
    "    output_dir: path to output directory containing fasta file\n",
    "    fasta_name: name of fasta file, minus the .fasta/.fa file ending\n",
    "    computed: path to already computed .ss2 file to skip re-running PSIPRED\n",
    "    ''' \n",
    "    if computed is not None:\n",
    "        return [paddle.read_ss2(computed)], adpred_read_ss2(computed)\n",
    "    \n",
    "    p = subprocess.run('cd \"{}\" && {} \"{}\".fa*'.format(output_dir, local_psipred, fasta_name), \n",
    "                       capture_output=True, shell=True, encoding='utf-8') \n",
    "    rootname = re.search('Final output files: (.*).ss2 (.*).horiz', p.stdout).group(1).strip()\n",
    "    \n",
    "    paddle_ss = paddle.read_ss2(output_dir + '/' + rootname + '.ss2')\n",
    "    f = open(output_dir + '/' + rootname + '.horiz')\n",
    "    adpred_ss = ''.join([i.group(1) for i in re.finditer('Pred: (.*)\\n', ''.join(f))]).replace(\"C\",\"-\")\n",
    "\n",
    "    return paddle_ss, adpred_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddle_ss, adpred_ss = get_psipred(sequence)\n",
    "assert paddle_ss[0] == sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PADDLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PADDLE needs disorder predictions from IUPred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iupred_single(seq, output_dir=output_dir, fasta_name=fasta_name, computed_long=None, computed_short=None):\n",
    "    '''\n",
    "    Runs IUPred on the provided sequence and returns\n",
    "    sequence, list of long-style predicted disorder values, list of short-style predicted disorder values\n",
    "    \n",
    "    seq: provided sequence\n",
    "    output_dir: path to output directory\n",
    "    fasta_name: path to fasta file\n",
    "    computed_long: path to already computed long-style .dis file to skip re-running IUPred\n",
    "    computed_short: path to already computed short-style .dis file to skip re-running IUPred\n",
    "    ''' \n",
    "    if computed_long is not None and computed_short is not None:\n",
    "        seq, long_out = read_iupred_dict(computed_long)\n",
    "        seq, short_out = read_iupred_dict(computed_short)\n",
    "        return seq, long_out, short_out\n",
    "    \n",
    "    with open(output_dir + '/' + fasta_name + '_long.dis', 'w') as dis:\n",
    "        dis.write(get_iupred(seq, iupred_type='long'))\n",
    "    with open(output_dir + '/' + fasta_name + '_short.dis', 'w') as dis:\n",
    "        dis.write(get_iupred(seq, iupred_type='short'))\n",
    "    \n",
    "    seq, long_out = read_iupred(output_dir + '/' + fasta_name + '_long.dis')\n",
    "    seq, short_out = read_iupred(output_dir + '/' + fasta_name + '_short.dis')\n",
    "    \n",
    "    return seq, long_out, short_out\n",
    "            \n",
    "            \n",
    "def read_iupred(file):\n",
    "    \"\"\"\n",
    "    Read disorder predictions produced by IUPRED2 from the .dis file\n",
    "    Adapted from PADDLE's native read_iupred fn due to formatting\n",
    "    Inputs:\n",
    "        - file: Filename of the .dis file output by IUPRED2\n",
    "    Returns:\n",
    "        - prot: Protein sequence (string)\n",
    "        - dis:  List of predicted disorder (values between 0 and 1)\n",
    "    \"\"\"\n",
    "\n",
    "    f = open(file, 'r')\n",
    "    dis = []\n",
    "    prot = ''\n",
    "\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if '#' in line or line == \"\":\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        assert len(tokens) == 3, tokens\n",
    "\n",
    "        prot += tokens[1]\n",
    "        dis.append(float(tokens[2]))\n",
    "\n",
    "    f.close()\n",
    "    return prot, dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, dis_long, dis_short = read_iupred_single(sequence)\n",
    "assert seq == sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Run predictions using PADDLE. This network requires predicted secondary\n",
    "# structure (from PSIPRED) and predicted disorder (from IUPRED2, in both the\n",
    "# short and long modes) as input in addition to the protein sequence. This\n",
    "# model is the most accurate and should be used for predicted ADs in wild-type\n",
    "# proteins.\n",
    "########\n",
    "\n",
    "# Load PADDLE models. There are 10 in total, and their individual predictions\n",
    "# are averaged to obtain the final result.\n",
    "pad = paddle.PADDLE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-computed secondary structure predicted by PSIPRED V4.\n",
    "# When predicting across a large number of sequences, PSIPRED can be run\n",
    "# without using BLAST, this speeds up secondary structure prediction.\n",
    "# These PSIPRED predictions have been run with BLAST\n",
    "prot, helix, coil = paddle_ss[0], paddle_ss[1], paddle_ss[2]\n",
    "\n",
    "# Load pre-computed disorder predicted by IUPRED2, in both\n",
    "# the short and long modes.\n",
    "dis_short, dis_long = dis_short, dis_long\n",
    "\n",
    "# Run predictions on all 53 amino acid long tiles across the protein.\n",
    "# This function requires matching protein sequence and secondary structure scores.\n",
    "# Returns a Numpy array of size (protein_length-52) which gives the\n",
    "# predicted activation Z-score for the 53aa tiles starting at positions\n",
    "# 1, 2, 3, ..., protein_length-52.\n",
    "# High-strength ADs can be called by finding >=5 consecutive positions with Z-score > 6.\n",
    "# Medium-strength ADs can be called by finding >=5 consecutive positions with Z-score > 4.\n",
    "paddle_preds = pad.predict_protein(prot, helix, coil, dis_short, dis_long)\n",
    "paddle_centers = np.arange(len(paddle_preds)) + (53+1)/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adp_prot = adp.protein('x', sequence)\n",
    "# Load pre-computed secondary structure predicted by PSIPRED V4.\n",
    "adp_prot.second_struct = adpred_ss\n",
    "# Run ADpred\n",
    "adp_prot.predict()\n",
    "adpred_preds = adp_prot.predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADHunter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chop sequence into 40-mer tiles\n",
    "seqs = np.array([sequence[i:i+40] for i in range(len(sequence)-39)], dtype=object)\n",
    "\n",
    "# convert seqs to integer valued vectors\n",
    "alphabet=\"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_to_i = {aa:i for i, aa in enumerate(alphabet)}\n",
    "i_to_aa = {i:aa for i, aa in enumerate(alphabet)}\n",
    "X = np.asarray([[aa_to_i[aa] for aa in x] for x in seqs])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ADHunter model, trained on GCN4 ortholog set\n",
    "model = torch.load('/global/scratch/projects/fc_mvslab/predictors/adhunter/adhunter.pt')\n",
    "scaler = pickle.load(open('/global/scratch/projects/fc_mvslab/predictors/adhunter/scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "model.eval()\n",
    "X_in = torch.tensor(X)\n",
    "with torch.no_grad():\n",
    "    # Run ADHunter\n",
    "    y_in_hat = model(X_in).reshape(-1)\n",
    "    y_in_hat = y_in_hat.detach().numpy()\n",
    "\n",
    "# Convert model output to interpretable predicted activity values on the scale of the dataset used to train the model\n",
    "y_in_unscaled = scaler.inverse_transform(y_in_hat.reshape(-1, 1)).reshape(-1)\n",
    "print(y_in_unscaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhunter_preds = y_in_unscaled\n",
    "adhunter_centers = np.arange(len(adhunter_preds)) + 40/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chop sequence into 40-mer tiles\n",
    "seqs = np.array([sequence[i:i+40] for i in range(len(sequence)-39)], dtype=object)\n",
    "\n",
    "#Calculate features\n",
    "\n",
    "# Defines the sequence window size and steps (stride length). Change values if needed.\n",
    "SEQUENCE_WINDOW = 5\n",
    "STEPS = 1\n",
    "LENGTH = 40\n",
    "\n",
    "features = create_features(seqs, SEQUENCE_WINDOW, STEPS)\n",
    "features_scaled = scale_features(features)\n",
    "\n",
    "# Save the features\n",
    "dump(features_scaled, open(output_dir + '/features_scaled.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make TADA classification predictions\n",
    "tada_preds = tada_predict(output_dir + '/features_scaled.pkl')\n",
    "tada_centers = np.arange(len(tada_preds)) + 40/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traces and Prediction values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model uses a different scaling for their activity values and has different thresholds for whether or not a region is active, marked by the horizontal lines in the traces. PADDLE and ADHunter predicted values are normalized for easier comparison. Moreover, PADDLE predicts on 53-residue tiles of the provided sequence and outputs one activity value per tile, ADHunter and TADA predict on 40-residue tiles of the provided sequence and output one activity value per tile, and ADpred predicts on 30-residue tiles and outputs one activity probability value for every residue in the provided sequence. ADpred pads shorter tiles at the N-terminal end of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_paddle(arr):\n",
    "    arr = np.array(arr)\n",
    "    return arr/12\n",
    "\n",
    "def normalize_adhunter(arr):\n",
    "    arr = np.array(arr)\n",
    "    return arr/239716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = matplotlib.backends.backend_pdf.PdfPages(output_dir + \"/trace.pdf\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# Plot activity traces for all three predictors and their activity thresholds\n",
    "ax.plot(paddle_centers, normalize_paddle(paddle_preds), c='b', label='PADDLE')\n",
    "ax.plot(np.arange(len(sequence)), adpred_preds, c='orange', label='ADPred')\n",
    "ax.plot(adhunter_centers, normalize_adhunter(adhunter_preds), c='green', label='ADHunter')\n",
    "ax.plot(tada_centers, tada_preds, c='red', label='TADA')\n",
    "ax.plot([0, len(sequence)-1], [.8, .8], '-', c='orange')\n",
    "ax.plot([0, len(sequence)-1], [4/12, 4/12], '-', c='b')\n",
    "ax.plot([0, len(sequence)-1], [100000/239716, 100000/239716], '-', c='green')\n",
    "ax.plot([0, len(sequence)-1], [.5, .5], '-', c='red')\n",
    "\n",
    "ax.set_title(fasta_name)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.margins(x=0)\n",
    "\n",
    "ax.set_xlabel('Center position of tile')\n",
    "ax.set_ylabel('Activity scores, normalized')\n",
    "\n",
    "# Plot protein sequence on a secondary x-axis\n",
    "# Comment out this block of code if it looks too messy for you\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xticks(np.arange(len(sequence)))\n",
    "ax2.set_xticklabels([aa for aa in sequence], fontsize=3)\n",
    "\n",
    "pdf.savefig()\n",
    "plt.show()\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted values to a csv file\n",
    "with open(output_dir + '/preds.csv', 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['sequence', 'paddle_preds', 'adpred_preds', 'adhunter_preds', 'tada_preds'])\n",
    "    w.writerow([sequence, paddle_preds.tolist(), adpred_preds.tolist(), adhunter_preds.tolist(), tada_preds.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(output_dir + '/preds.csv', converters={'paddle_preds':pd.eval, 'adpred_preds':pd.eval, 'adhunter_preds':pd.eval, 'tada_preds':pd.eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH mode begins here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please delete/move files in tmp_output after you're done. Files in tmp_output should not be expected to stay there permanently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipp.__version__   # check that at least version 7\n",
    "mycluster = ipp.Cluster(n = int(os.getenv('SLURM_CPUS_ON_NODE')))\n",
    "mycluster.start_cluster_sync()\n",
    "\n",
    "c = mycluster.connect_client_sync()\n",
    "c.wait_for_engines(n = int(os.getenv('SLURM_CPUS_ON_NODE')))\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview = c[:]\n",
    "dview.block = True\n",
    "\n",
    "# Load necessary modules on workers\n",
    "dview.execute('import sys, os')\n",
    "dview.execute('sys.path.append(os.path.abspath(\"..\"))')\n",
    "dview.execute('import pandas as pd')\n",
    "dview.execute('import numpy as np')\n",
    "dview.execute('import re, subprocess')\n",
    "dview.execute('import Bio')\n",
    "dview.execute('from PADDLE import paddle')\n",
    "dview.execute('from adpred import ADpred as adp')\n",
    "dview.execute('import torch')\n",
    "dview.execute('from TADA.src.Preprocessing import scale_features')\n",
    "dview.execute('from TADA.src.Preprocessing import create_features')\n",
    "dview.execute('from TADA.src.Predict_model_only import tada_predict')\n",
    "dview.execute('from TADA.src.Predict_model_only import load_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the path to a fasta file containing your sequences\n",
    "\n",
    "# INPUT PATH TO FASTA FILE HERE\n",
    "fasta_name = '/global/scratch/projects/fc_mvslab/predictors/all_predictors/HcG217B_Ryp1.fasta'\n",
    "\n",
    "tmp_output = '/global/scratch/projects/fc_mvslab/predictors/all_predictors/tmp_output'\n",
    "\n",
    "aa_lst = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "\n",
    "# Set up directories for storing output, including PSIPRED and IUPred predictions\n",
    "fasta_name_tmp = fasta_name.split('/')[-1].strip('.fasta').strip('.fa')\n",
    "output_dir = tmp_output + '/' + fasta_name_tmp\n",
    "subprocess.run(['mkdir', output_dir])\n",
    "recs = list(SeqIO.parse(fasta_name, 'fasta'))\n",
    "fasta_name = fasta_name_tmp\n",
    "for r in recs:\n",
    "    sequence = str(r.seq)\n",
    "    assert len(sequence) >= 53, 'Protein sequence must be at least 53 amino acids long' \n",
    "    assert len(set(sequence) - set(aa_lst)) == 0, 'No non-standard amino acids allowed'\n",
    "    sub_fasta_name = re.sub(r'\\s+', '_', r.id)\n",
    "    subprocess.run(['mkdir', output_dir + '/' + sub_fasta_name])\n",
    "    with open(output_dir + '/' + sub_fasta_name + '/' + sub_fasta_name + '.fasta', 'w') as fa:\n",
    "        fa.write(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSIPRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both PADDLE and ADpred need secondary structure predictions from PSIPRED. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to a local installation of psipred\n",
    "local_psipred = '/global/scratch/projects/fc_mvslab/predictors/psipred/runpsipred'\n",
    "\n",
    "\n",
    "def adpred_read_ss2(ss2):\n",
    "    '''\n",
    "    Converts .ss2 data to horizontal secondary structure information (e.g., --HHHHH--HHHHH-HH)\n",
    "    \n",
    "    ss2: name of .ss2 file\n",
    "    '''\n",
    "    df = pd.read_table(ss2, skiprows=[0], sep='\\s+', names=['position', 'residue', 'ss', 'c', 'h', 'e'])\n",
    "    return ''.join(df.ss.values).replace('C','-')\n",
    "\n",
    "def get_psipred(seq, output_dir=output_dir, fasta_name=fasta_name, computed=None):\n",
    "    '''\n",
    "    Runs PSIPRED on the provided sequence and returns secondary structure\n",
    "    data for PADDLE and ADpred, viz\n",
    "    (paddle_seq, paddle_helix, paddle_coil), 'adpred_ss_as_a_string'\n",
    "    \n",
    "    seq: provided sequence\n",
    "    output_dir: path to output directory containing fasta file\n",
    "    fasta_name: name of fasta file, minus the .fasta/.fa file ending\n",
    "    computed: path to already computed .ss2 file to skip re-running PSIPRED\n",
    "    ''' \n",
    "    if computed is not None:\n",
    "        return paddle.read_ss2(computed), adpred_read_ss2(computed)\n",
    "    \n",
    "    p = subprocess.run('cd \"{}\" && {} \"{}\".fa*'.format(output_dir, local_psipred, fasta_name), \n",
    "                       capture_output=True, shell=True, encoding='utf-8')\n",
    "    rootname = re.search('Final output files: (.*).ss2 (.*).horiz', p.stdout).group(1).strip()\n",
    "    \n",
    "    paddle_ss = paddle.read_ss2(output_dir + '/' + rootname + '.ss2')\n",
    "    f = open(output_dir + '/' + rootname + '.horiz')\n",
    "    adpred_ss = ''.join([i.group(1) for i in re.finditer('Pred: (.*)\\n', ''.join(f))]).replace(\"C\",\"-\")\n",
    "\n",
    "    return paddle_ss, adpred_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dview = c[:]\n",
    "dview.block = True\n",
    "\n",
    "# Push variables and functions to workers\n",
    "var = dict( output_dir=output_dir, local_psipred=local_psipred, fasta_name=fasta_name, adpred_read_ss2=adpred_read_ss2, get_psipred=get_psipred )\n",
    "dview.push(var)\n",
    "\n",
    "# Need a wrapper function because map() only operates on one argument\n",
    "def wrapper(r):\n",
    "    sequence = str(r.seq)\n",
    "    sub_fasta_name = re.sub(r'\\s+', '_', r.id)\n",
    "    paddle_ss, adpred_ss = get_psipred(sequence, output_dir=output_dir + '/' + sub_fasta_name, fasta_name=sub_fasta_name,)\n",
    "                                      #computed=\"{}/{}/{}.ss2\".format(output_dir, sub_fasta_name, sub_fasta_name) )\n",
    "    assert paddle_ss[0] == sequence\n",
    "    return (sequence, [paddle_ss, adpred_ss])\n",
    "\n",
    "# Run a parallel map, executing the wrapper function on indices 0,...,n-1\n",
    "lview = c.load_balanced_view()\n",
    "# Cause execution on main process to wait while tasks sent to workers finish\n",
    "lview.block = True \n",
    "out = lview.map(wrapper, recs)   # Run calculation in parallel\n",
    "ss_dict = dict(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PADDLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PADDLE needs disorder predictions from IUPred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iupred_batch(recs, output_dir=output_dir, fasta_name=fasta_name, computed_long=None, computed_short=None):\n",
    "    '''\n",
    "    Runs IUPred on the provided sequences and returns two dictionaries:\n",
    "    {sequences: lists of long-style predicted disorder values} and\n",
    "    {sequences: lists of short-style predicted disorder values}\n",
    "    \n",
    "    recs: list of Bio.SeqRecord objects containing the provided sequences\n",
    "    output_dir: path to output directory\n",
    "    fasta_name: path to fasta file\n",
    "    computed_long: path to already computed long-style .dis file to skip re-running IUPred\n",
    "    computed_short: path to already computed short-style .dis file to skip re-running IUPred\n",
    "    ''' \n",
    "    if computed_long is not None and computed_short is not None:\n",
    "        return read_iupred_dict(computed_long), read_iupred_dict(computed_short)\n",
    "    \n",
    "    with open(output_dir + '/' + fasta_name + '_long.dis', 'w') as dis:\n",
    "        for r in recs:\n",
    "            dis.write('> ' + r.id + '\\n')\n",
    "            dis.write(get_iupred(str(r.seq), iupred_type='long'))\n",
    "            dis.write('\\n\\n')\n",
    "    with open(output_dir + '/' + fasta_name + '_short.dis', 'w') as dis:\n",
    "        for r in recs:\n",
    "            dis.write('> ' + r.id + '\\n')\n",
    "            dis.write(get_iupred(str(r.seq), iupred_type='short'))\n",
    "            dis.write('\\n\\n')\n",
    "    \n",
    "    long_out = read_iupred_dict(output_dir + '/' + fasta_name + '_long.dis')\n",
    "    short_out = read_iupred_dict(output_dir + '/' + fasta_name + '_short.dis')\n",
    "    \n",
    "    return long_out, short_out\n",
    "\n",
    "\n",
    "def read_iupred_dict(file):\n",
    "    \"\"\"\n",
    "    Read disorder predictions produced by IUPRED2 from the .dis file\n",
    "    Adapted from PADDLE's native read_iupred fn due to formatting\n",
    "    Inputs:\n",
    "        - file: Filename of the .dis file output by IUPRED2\n",
    "    Returns:\n",
    "        - iupred_dict: Dictionary of prot keys and dis values\n",
    "        where\n",
    "        - prot: Protein sequence (string)\n",
    "        - dis:  List of predicted disorder (values between 0 and 1)\n",
    "    \"\"\"\n",
    "\n",
    "    f = open(file, 'r')\n",
    "    iupred_dict = dict()\n",
    "    dis = []\n",
    "    prot = ''\n",
    "\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith('>'):\n",
    "            iupred_dict[prot] = dis\n",
    "            dis = []\n",
    "            prot = ''\n",
    "            continue\n",
    "        if '#' in line or line == \"\":\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        assert len(tokens) == 3, tokens\n",
    "\n",
    "        prot += tokens[1]\n",
    "        dis.append(float(tokens[2]))\n",
    "\n",
    "    iupred_dict[prot] = dis\n",
    "    iupred_dict.pop(\"\")\n",
    "\n",
    "    f.close()\n",
    "    return iupred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_long_dict, dis_short_dict = read_iupred_batch(recs,)\n",
    "                                                  #computed_long=\"{}/{}_long.dis\".format(output_dir, fasta_name),\n",
    "                                                  #computed_short=\"{}/{}_short.dis\".format(output_dir, fasta_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "########\n",
    "# Run predictions using PADDLE. This network requires predicted secondary\n",
    "# structure (from PSIPRED) and predicted disorder (from IUPRED2, in both the\n",
    "# short and long modes) as input in addition to the protein sequence. This\n",
    "# model is the most accurate and should be used for predicted ADs in wild-type\n",
    "# proteins.\n",
    "########\n",
    "\n",
    "# Load PADDLE models. There are 10 in total, and their individual predictions\n",
    "# are averaged to obtain the final result.\n",
    "dview.execute('pad=paddle.PADDLE()')\n",
    "\n",
    "# Push PSIPRED and IUPred output to workers\n",
    "dview.push(dict( ss_dict=ss_dict, dis_long_dict=dis_long_dict, dis_short_dict=dis_short_dict ))\n",
    "\n",
    "# Need a wrapper function because map() only operates on one argument\n",
    "def wrapper(r):\n",
    "    seq = str(r.seq)\n",
    "    \n",
    "    # Load pre-computed secondary structure predicted by PSIPRED V4.\n",
    "    # When predicting across a large number of sequences, PSIPRED can be run\n",
    "    # without using BLAST, this speeds up secondary structure prediction.\n",
    "    # These PSIPRED predictions have been run with BLAST\n",
    "    paddle_ss = ss_dict[seq][0]\n",
    "    prot, helix, coil = paddle_ss[0], paddle_ss[1], paddle_ss[2]\n",
    "\n",
    "    # Load pre-computed disorder predicted by IUPRED2, in both\n",
    "    # the short and long modes.\n",
    "    dis_short, dis_long = dis_short_dict[seq], dis_long_dict[seq]\n",
    "\n",
    "    # Run predictions on all 53 amino acid long tiles across the protein.\n",
    "    # This function requires matching protein sequence and secondary structure scores.\n",
    "    # Returns a Numpy array of size (protein_length-52) which gives the\n",
    "    # predicted activation Z-score for the 53aa tiles starting at positions\n",
    "    # 1, 2, 3, ..., protein_length-52.\n",
    "    # High-strength ADs can be called by finding >=5 consecutive positions with Z-score > 6.\n",
    "    # Medium-strength ADs can be called by finding >=5 consecutive positions with Z-score > 4.\n",
    "    paddle_preds = pad.predict_protein(prot, helix, coil, dis_short, dis_long)\n",
    "    paddle_centers = np.arange(len(paddle_preds)) + (53+1)/2\n",
    "    \n",
    "    return (seq, (paddle_centers, paddle_preds))\n",
    "\n",
    "\n",
    "# Run a parallel map, executing the wrapper function on indices 0,...,n-1\n",
    "lview = c.load_balanced_view()\n",
    "# Cause execution on main process to wait while tasks sent to workers finish\n",
    "lview.block = True \n",
    "out = lview.map(wrapper, recs)   # Run calculation in parallel\n",
    "paddle_out = dict(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Need a wrapper function because map() only operates on one argument\n",
    "def wrapper(r):\n",
    "    seq = str(r.seq)\n",
    "    prot = adp.protein('x', seq)\n",
    "    \n",
    "    # Load horizontal secondary structure data\n",
    "    prot.second_struct = ss_dict[seq][1]\n",
    "    \n",
    "    # Run ADpred\n",
    "    prot.predict()\n",
    "    return (prot.sequence, prot.predictions)\n",
    "\n",
    "\n",
    "# Run a parallel map, executing the wrapper function on indices 0,...,n-1\n",
    "lview = c.load_balanced_view()\n",
    "# Cause execution on main process to wait while tasks sent to workers finish\n",
    "lview.block = True \n",
    "out = lview.map(wrapper, recs)   # Run calculation in parallel\n",
    "adpred_out = dict(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADHunter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sequence):\n",
    "    '''\n",
    "    Chops the sequence into 40-mer tiles and converts them to integer valued vectors.\n",
    "    Returns the one-hot-encoded 40-mers as a tensor\n",
    "    \n",
    "    sequence: provided sequence\n",
    "    output_dir: path to output directory\n",
    "    fasta_name: path to fasta file\n",
    "    computed: path to already computed .dis file to skip re-running IUPred\n",
    "    ''' \n",
    "    # Chop sequence into 40-mer tiles\n",
    "    seqs = np.array([sequence[i:i+40] for i in range(len(sequence)-39)], dtype=object)\n",
    "\n",
    "    # convert seqs to integer valued vectors\n",
    "    alphabet=\"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    aa_to_i = {aa:i for i, aa in enumerate(alphabet)}\n",
    "    i_to_aa = {i:aa for i, aa in enumerate(alphabet)}\n",
    "    X = np.asarray([[aa_to_i[aa] for aa in x] for x in seqs])\n",
    "\n",
    "    # Convert matrix to tensor\n",
    "    return torch.tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ADHunter model, trained on GCN4 ortholog set\n",
    "model = torch.load('/global/scratch/projects/fc_mvslab/predictors/adhunter/adhunter.pt')\n",
    "model.eval()\n",
    "scaler = pickle.load(open('/global/scratch/projects/fc_mvslab/predictors/adhunter/scaler.pkl', 'rb'))\n",
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dview.push( dict(model=model, scaler=scaler, preprocessing=preprocessing) )\n",
    "\n",
    "# Need a wrapper function because map() only operates on one argument\n",
    "def wrapper(r):\n",
    "    seq = str(r.seq)\n",
    "    X = preprocessing(seq)\n",
    "    with torch.no_grad():\n",
    "        # Run ADHunter\n",
    "        y_hat = model(X).reshape(-1)\n",
    "        y_hat = y_hat.detach().numpy()\n",
    "        \n",
    "    # Convert model output to interpretable predicted activity values on the scale of the dataset used to train the model\n",
    "    y_unscaled = scaler.inverse_transform(y_hat.reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    adhunter_preds = y_unscaled\n",
    "    adhunter_centers = np.arange(len(adhunter_preds)) + 40/2\n",
    "    \n",
    "    return (seq, (adhunter_centers, adhunter_preds))\n",
    "\n",
    "\n",
    "# Run a parallel map, executing the wrapper function on indices 0,...,n-1\n",
    "lview = c.load_balanced_view()\n",
    "# Cause execution on main process to wait while tasks sent to workers finish\n",
    "lview.block = True \n",
    "out = lview.map(wrapper, recs)   # Run calculation in parallel\n",
    "adhunter_out = dict(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TADA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slowest component of running TADA is feature generation. It does not parallelize well and needs to be run single-threaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Defines the sequence window size and steps (stride length). Change values if needed.\n",
    "SEQUENCE_WINDOW = 5\n",
    "STEPS = 1\n",
    "LENGTH = 40\n",
    "\n",
    "seqs_40 = []\n",
    "seqs_idx = 0\n",
    "tada_out = dict()\n",
    "\n",
    "#Iterate through all sequences\n",
    "for r in recs:\n",
    "    seq = str(r.seq)\n",
    "    \n",
    "    # Chop sequence into 40-mer tiles\n",
    "    seqs = np.array([seq[i:i+40] for i in range(len(seq)-39)], dtype=object)\n",
    "    seqs_40.append(seqs)\n",
    "    \n",
    "    # Record indices of tiles corresponding to the sequence\n",
    "    tada_out[seq] = np.arange(seqs_idx, seqs_idx+len(seqs))\n",
    "    seqs_idx += len(seqs)\n",
    "\n",
    "# Flatten list of 40-mer tiles\n",
    "seqs_40 = np.concatenate(seqs_40)\n",
    "\n",
    "assert seqs_idx == len(seqs_40)\n",
    "\n",
    "#Calculate features on 40-mer tiles\n",
    "features = create_features(seqs_40, SEQUENCE_WINDOW, STEPS)\n",
    "features_scaled = scale_features(features)\n",
    "pickle.dump(features_scaled, open(output_dir + '/features_scaled.pkl', 'wb'))\n",
    "    \n",
    "# Make TADA classification predictions\n",
    "tada_preds = tada_predict(loaded_features=features_scaled)\n",
    "\n",
    "# Use indices of tiles to retrieve predictions for each sequence\n",
    "for seq, val in tada_out.items():\n",
    "    preds = tada_preds[val]\n",
    "    centers = np.arange(len(preds)) + 40/2\n",
    "    tada_out[seq] = (centers, preds)\n",
    "\n",
    "subprocess.run(['rm', 'scaler_minmax_features.pkl'])\n",
    "subprocess.run(['rm', 'scaler_normal_features.pkl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traces and Prediction values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model uses a different scaling for their activity values and has different thresholds for whether or not a region is active, marked by the horizontal lines in the traces. PADDLE and ADHunter predicted values are normalized for easier comparison. Moreover, PADDLE predicts on 53-residue tiles of the provided sequence and outputs one activity value per tile, ADHunter and TADA predict on 40-residue tiles of the provided sequence and output one activity value per tile, and ADpred predicts on 30-residue tiles and outputs one activity probability value for every residue in the provided sequence. ADpred pads shorter tiles at the N-terminal end of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_paddle(arr):\n",
    "    arr = np.array(arr)\n",
    "    return arr/12\n",
    "\n",
    "def normalize_adhunter(arr):\n",
    "    arr = np.array(arr)\n",
    "    return arr/239716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = matplotlib.backends.backend_pdf.PdfPages(output_dir + \"/traces.pdf\")\n",
    "\n",
    "for r in recs:\n",
    "    sequence = str(r.seq)\n",
    "    paddle_centers, paddle_preds = paddle_out[sequence]\n",
    "    adpred_preds = adpred_out[sequence]\n",
    "    adhunter_centers, adhunter_preds = adhunter_out[sequence]\n",
    "    tada_centers, tada_preds = tada_out[sequence]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "    # Plot activity traces for all three predictors and their activity thresholds\n",
    "    ax.plot(paddle_centers, normalize_paddle(paddle_preds), c='b', label='PADDLE')\n",
    "    ax.plot(np.arange(len(sequence)), adpred_preds, c='orange', label='ADPred')\n",
    "    ax.plot(adhunter_centers, normalize_adhunter(adhunter_preds), c='green', label='ADHunter')\n",
    "    ax.plot(tada_centers, tada_preds, c='red', label='TADA')\n",
    "    ax.plot([0, len(sequence)-1], [.8, .8], '-', c='orange')\n",
    "    ax.plot([0, len(sequence)-1], [4/12, 4/12], '-', c='b')\n",
    "    ax.plot([0, len(sequence)-1], [100000/239716, 100000/239716], '-', c='green')\n",
    "    ax.plot([0, len(sequence)-1], [.5, .5], '-', c='red')\n",
    "\n",
    "    ax.set_title(r.id)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.margins(x=0)\n",
    "\n",
    "    ax.set_xlabel('Center position of tile')\n",
    "    ax.set_ylabel('Activity scores, normalized')\n",
    "\n",
    "    # Plot protein sequence on a secondary x-axis\n",
    "    # Comment out this block of code if it looks too messy for you\n",
    "    ax2 = ax.twiny()\n",
    "    ax2.set_xticks(np.arange(len(sequence)))\n",
    "    ax2.set_xticklabels([aa for aa in sequence], fontsize=3)\n",
    "\n",
    "    pdf.savefig()\n",
    "    plt.show()\n",
    "    \n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted values to a csv file\n",
    "data = []\n",
    "for r in recs:\n",
    "    sequence = str(r.seq)\n",
    "    paddle_centers, paddle_preds = paddle_out[sequence]\n",
    "    adpred_preds = adpred_out[sequence]\n",
    "    adhunter_centers, adhunter_preds = adhunter_out[sequence]\n",
    "    tada_centers, tada_preds = tada_out[sequence]\n",
    "    \n",
    "    data.append([sequence, paddle_centers.tolist(), paddle_preds.tolist(), adpred_preds.tolist(), \n",
    "                 adhunter_centers.tolist(), adhunter_preds.tolist(), tada_centers.tolist(), tada_preds.tolist()])\n",
    "\n",
    "out_df = pd.DataFrame(data, columns=['sequence', 'paddle_centers', 'paddle_preds', 'adpred_preds', 'adhunter_centers', 'adhunter_preds', 'tada_centers', 'tada_preds'])\n",
    "out_df.to_csv(output_dir + \"/preds.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(output_dir + '/preds.csv', converters={arr:pd.eval for arr in ['paddle_centers', 'paddle_preds', 'adpred_preds', 'adhunter_centers', 'adhunter_preds', 'tada_centers', 'tada_preds']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
